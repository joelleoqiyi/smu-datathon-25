{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Loaded dataset with 1509 rows.\n",
      "ðŸ”„ Loading sentence embedding model for category mapping...\n",
      "âœ… Model loaded successfully.\n",
      "ðŸ”Ž Processing excerpt 1/1509...\n",
      "ðŸ“¤ Sending request to GPT-4o...\n",
      "âœ… GPT-4o response received.\n",
      "ðŸ”„ New relationship type detected and added: family\n",
      "ðŸ”„ New relationship type detected and added: residence\n",
      "ðŸ”„ New relationship type detected and added: employment\n",
      "ðŸ”„ Grouped 'location' under 'geographical_association'\n",
      "ðŸ”Ž Processing excerpt 2/1509...\n",
      "ðŸ“¤ Sending request to GPT-4o...\n",
      "âœ… GPT-4o response received.\n",
      "ðŸ”„ Grouped 'location' under 'geographical_association'\n",
      "ðŸ”Ž Processing excerpt 3/1509...\n",
      "ðŸ“¤ Sending request to GPT-4o...\n",
      "âœ… GPT-4o response received.\n",
      "ðŸ”„ Grouped 'location' under 'geographical_association'\n",
      "ðŸ”Ž Processing excerpt 4/1509...\n",
      "ðŸ“¤ Sending request to GPT-4o...\n",
      "âœ… GPT-4o response received.\n",
      "ðŸ”„ New relationship type detected and added: founder\n",
      "ðŸ”„ New relationship type detected and added: leadership\n",
      "ðŸ”„ New relationship type detected and added: successor\n",
      "ðŸ”„ New relationship type detected and added: product\n",
      "ðŸ”„ New relationship type detected and added: marriage\n",
      "ðŸ”„ New relationship type detected and added: affiliation\n",
      "ðŸ”Ž Processing excerpt 5/1509...\n",
      "ðŸ“¤ Sending request to GPT-4o...\n",
      "âœ… GPT-4o response received.\n",
      "ðŸ”„ Grouped 'marital' under 'marriage'\n",
      "ðŸ”„ New relationship type detected and added: customer\n",
      "ðŸ”„ Grouped 'location' under 'geographical_association'\n",
      "âœ… Relationship extraction complete! Results saved to results/extracted_relationships_jcc.json\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "import pandas as pd\n",
    "import json\n",
    "import time\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "client = openai.OpenAI(api_key=os.getenv(\"API_KEY\"))\n",
    "\n",
    "# âœ… LOAD THE DATASET\n",
    "file_path = \"results/generalized_normalized_ner_with_embeddings.xlsx\"  # Update with your actual file path\n",
    "try:\n",
    "    df = pd.read_excel(file_path)\n",
    "    print(f\"âœ… Loaded dataset with {len(df)} rows.\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error loading dataset: {e}\")\n",
    "    exit()\n",
    "\n",
    "# âœ… LOAD SEMANTIC SIMILARITY MODEL\n",
    "print(\"ðŸ”„ Loading sentence embedding model for category mapping...\")\n",
    "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "print(\"âœ… Model loaded successfully.\")\n",
    "\n",
    "# âœ… PREDEFINED RELATIONSHIP CATEGORIES\n",
    "standard_relationships = {\n",
    "    \"legal_violation\": [\"legal\", \"lawsuit\", \"court_case\", \"regulatory_violation\"],\n",
    "    \"corporate_action\": [\"business_decision\", \"merger\", \"acquisition\"],\n",
    "    \"government_regulation\": [\"regulation\", \"policy\", \"law_enforcement\"],\n",
    "    \"geographical_association\": [\"location\", \"place\", \"geographical\"],\n",
    "    \"crime_involvement\": [\"criminal\", \"crime\", \"fraud\", \"arrest\", \"indictment\"],\n",
    "    \"financial_transaction\": [\"finance\", \"funding\", \"money_transfer\", \"seizure\", \"investment\"],\n",
    "    \"scientific_discovery\": [\"science\", \"discovery\", \"innovation\", \"technology_breakthrough\"],\n",
    "    \"historical_event\": [\"history\", \"historical_event\", \"war\", \"political_event\"]\n",
    "}\n",
    "\n",
    "# âœ… FUNCTION TO GROUP SIMILAR RELATIONSHIP TYPES\n",
    "def map_to_standard_relationship(relationship_type):\n",
    "    if not relationship_type:\n",
    "        return \"unknown\"\n",
    "\n",
    "    relationship_type = relationship_type.lower().replace(\"_\", \" \")\n",
    "    best_match = None\n",
    "    best_score = 0.7  \n",
    "\n",
    "    for standard, variations in standard_relationships.items():\n",
    "        variations.append(standard)  \n",
    "        embeddings1 = embedding_model.encode(relationship_type, convert_to_tensor=True)\n",
    "        embeddings2 = embedding_model.encode(variations, convert_to_tensor=True)\n",
    "\n",
    "        similarities = util.pytorch_cos_sim(embeddings1, embeddings2).squeeze().tolist()\n",
    "        max_similarity = max(similarities)\n",
    "\n",
    "        if max_similarity > best_score:\n",
    "            best_match = standard\n",
    "            best_score = max_similarity\n",
    "\n",
    "    if best_match is None:\n",
    "        best_match = relationship_type.replace(\" \", \"_\")\n",
    "        standard_relationships[best_match] = [relationship_type]\n",
    "        print(f\"ðŸ”„ New relationship type detected and added: {best_match}\")\n",
    "\n",
    "    return best_match\n",
    "\n",
    "# âœ… Function to format the prompt for GPT-4o\n",
    "def generate_prompt(text, entities):\n",
    "#     return f\"\"\"\n",
    "# You are an expert in **relationship extraction**. Your task is to extract and classify all possible relationships between named entities in the given text.\n",
    "\n",
    "# ---\n",
    "\n",
    "# ### ** Instructions:**\n",
    "# 1. **Extract ALL possible relationships**  \n",
    "#    - Identify **explicit** relationships (clearly stated in the text).  \n",
    "#    - Identify **implicit** relationships (that can be inferred based on real-world knowledge).  \n",
    "#    - If no meaningful relationship exists, **set `\"relationship_type\": \"null\"`**.\n",
    "\n",
    "# 2. **Format the Output as JSON (ONLY JSON):**  \n",
    "#    Each relationship should include:\n",
    "#    - `\"head\"`: The first entity in the relationship.\n",
    "#    - `\"tail\"`: The second entity in the relationship.\n",
    "#    - `\"relationship\"`: A clear description of how they are related.\n",
    "#    - `\"relationship_type\"`: The most suitable category (if unknown, still return it).\n",
    "\n",
    "# --- \n",
    "\n",
    "# Text to Analyze:{text}\n",
    "\n",
    "# Entities: {entities}\n",
    "\n",
    "# \"\"\"\n",
    "\n",
    "    return f\"\"\"\n",
    "    \n",
    "You are an expert in relationship extraction. Your task is to extract and classify all possible relationships between named entities in the given text.\n",
    "\n",
    "1. Extract ALL possible and unique relationships between ALL entity pairs  \n",
    "   - Identify explicit relationships (clearly stated in the text).  \n",
    "   - Identify implicit relationships (that can be inferred based on real-world knowledge).  \n",
    "   - If no meaningful relationship exists, set \"relationship_type\": \"null\"\n",
    "\n",
    "2. Format the Output as JSON (STRICTLY JSON):  \n",
    "   ```json\n",
    "   {{\n",
    "     \"relationships\": [\n",
    "       {{\n",
    "         \"head\": \"<Entity 1>\",\n",
    "         \"tail\": \"<Entity 2>\",\n",
    "         \"relationship\": \"<Description of relationship>\",\n",
    "         \"relationship_type\": \"<Categorized relationship type>\"\n",
    "       }},\n",
    "       ...\n",
    "     ]\n",
    "   }}\n",
    "    \"\"\"\n",
    "\n",
    "def extract_relationships_gpt(text, entities):\n",
    "    prompt = generate_prompt(text, entities)\n",
    "\n",
    "    try:\n",
    "        print(\"ðŸ“¤ Sending request to GPT-4o...\")\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o\",\n",
    "            messages=[{\"role\": \"system\", \"content\": prompt}],\n",
    "            temperature=0.3,\n",
    "            response_format={ \"type\": \"json_object\" }\n",
    "        )\n",
    "        response_content = response.choices[0].message.content\n",
    "        print(\"âœ… GPT-4o response received.\")\n",
    "\n",
    "        try:\n",
    "            extracted_relationships = json.loads(response_content)\n",
    "            return extracted_relationships\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(response_content)\n",
    "            print(f\"âŒ Error parsing GPT-4o response as JSON: {e}\")\n",
    "            return []\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ API request failed: {e}\")\n",
    "        return []\n",
    "\n",
    "results = []\n",
    "for index, row in df[0:5].iterrows():\n",
    "    text = row[\"Text\"]\n",
    "    entities = row[\"Normalized_NER_Entities\"]\n",
    "    if not text or not entities:\n",
    "        print(f\"âš ï¸ Skipping row {index + 1} due to missing data.\")\n",
    "        continue\n",
    "\n",
    "    print(f\"ðŸ”Ž Processing excerpt {index + 1}/{len(df)}...\")\n",
    "\n",
    "    relationships = extract_relationships_gpt(text, entities)\n",
    "\n",
    "    for rel in relationships['relationships']:\n",
    "        original_type = rel[\"relationship_type\"]\n",
    "        rel[\"relationship_type\"] = map_to_standard_relationship(original_type)\n",
    "        if original_type != rel[\"relationship_type\"]:\n",
    "            print(f\"ðŸ”„ Grouped '{original_type}' under '{rel['relationship_type']}'\")\n",
    "\n",
    "    results.append({\n",
    "        \"excerpt\": f\"Excerpt {index + 1}\",\n",
    "        \"relationships\": relationships\n",
    "    })\n",
    "\n",
    "    time.sleep(2)\n",
    "\n",
    "\n",
    "output_file = \"results/extracted_relationships_jcc.json\"\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(results, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "print(f\"âœ… Relationship extraction complete! Results saved to {output_file}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Loaded dataset with 1509 rows.\n",
      "ðŸ”„ Loading sentence embedding model for category mapping...\n",
      "âœ… Model loaded successfully.\n",
      "âœ… Skipping already processed: Excerpt 1\n",
      "âœ… Skipping already processed: Excerpt 2\n",
      "âœ… Skipping already processed: Excerpt 3\n",
      "âœ… Skipping already processed: Excerpt 4\n",
      "âœ… Skipping already processed: Excerpt 5\n",
      "âœ… Skipping already processed: Excerpt 6\n",
      "âœ… Skipping already processed: Excerpt 7\n",
      "âœ… Skipping already processed: Excerpt 8\n",
      "âœ… Skipping already processed: Excerpt 9\n",
      "âœ… Skipping already processed: Excerpt 10\n",
      "âœ… Skipping already processed: Excerpt 11\n",
      "âœ… Skipping already processed: Excerpt 12\n",
      "âœ… Skipping already processed: Excerpt 13\n",
      "âœ… Skipping already processed: Excerpt 14\n",
      "âœ… Skipping already processed: Excerpt 15\n",
      "âœ… Skipping already processed: Excerpt 16\n",
      "âœ… Skipping already processed: Excerpt 17\n",
      "âœ… Skipping already processed: Excerpt 18\n",
      "âœ… Skipping already processed: Excerpt 19\n",
      "âœ… Skipping already processed: Excerpt 20\n",
      "âœ… Skipping already processed: Excerpt 21\n",
      "âœ… Skipping already processed: Excerpt 22\n",
      "âœ… Skipping already processed: Excerpt 23\n",
      "âœ… Skipping already processed: Excerpt 24\n",
      "âœ… Skipping already processed: Excerpt 25\n",
      "âœ… Skipping already processed: Excerpt 26\n",
      "âœ… Skipping already processed: Excerpt 27\n",
      "âœ… Skipping already processed: Excerpt 28\n",
      "âœ… Skipping already processed: Excerpt 29\n",
      "âœ… Skipping already processed: Excerpt 30\n",
      "âœ… Skipping already processed: Excerpt 31\n",
      "âœ… Skipping already processed: Excerpt 32\n",
      "âœ… Skipping already processed: Excerpt 33\n",
      "âœ… Skipping already processed: Excerpt 34\n",
      "âœ… Skipping already processed: Excerpt 35\n",
      "âœ… Skipping already processed: Excerpt 36\n",
      "âœ… Skipping already processed: Excerpt 37\n",
      "âœ… Skipping already processed: Excerpt 38\n",
      "âœ… Skipping already processed: Excerpt 39\n",
      "âœ… Skipping already processed: Excerpt 40\n",
      "âœ… Skipping already processed: Excerpt 41\n",
      "âœ… Skipping already processed: Excerpt 42\n",
      "âœ… Skipping already processed: Excerpt 43\n",
      "âœ… Skipping already processed: Excerpt 44\n",
      "âœ… Skipping already processed: Excerpt 45\n",
      "âœ… Skipping already processed: Excerpt 46\n",
      "âœ… Skipping already processed: Excerpt 47\n",
      "âœ… Skipping already processed: Excerpt 48\n",
      "âœ… Skipping already processed: Excerpt 49\n",
      "âœ… Skipping already processed: Excerpt 50\n",
      "âœ… Skipping already processed: Excerpt 51\n",
      "âœ… Skipping already processed: Excerpt 52\n",
      "âœ… Skipping already processed: Excerpt 53\n",
      "âœ… Skipping already processed: Excerpt 54\n",
      "âœ… Skipping already processed: Excerpt 55\n",
      "âœ… Skipping already processed: Excerpt 56\n",
      "âœ… Skipping already processed: Excerpt 57\n",
      "âœ… Skipping already processed: Excerpt 58\n",
      "âœ… Skipping already processed: Excerpt 59\n",
      "âœ… Skipping already processed: Excerpt 60\n",
      "âœ… Skipping already processed: Excerpt 61\n",
      "âœ… Skipping already processed: Excerpt 62\n",
      "âœ… Skipping already processed: Excerpt 63\n",
      "âœ… Skipping already processed: Excerpt 64\n",
      "âœ… Skipping already processed: Excerpt 65\n",
      "âœ… Skipping already processed: Excerpt 66\n",
      "âœ… Skipping already processed: Excerpt 67\n",
      "âœ… Skipping already processed: Excerpt 68\n",
      "âœ… Skipping already processed: Excerpt 69\n",
      "âœ… Skipping already processed: Excerpt 70\n",
      "âœ… Skipping already processed: Excerpt 71\n",
      "âœ… Skipping already processed: Excerpt 72\n",
      "âœ… Skipping already processed: Excerpt 73\n",
      "âœ… Skipping already processed: Excerpt 74\n",
      "âœ… Skipping already processed: Excerpt 75\n",
      "âœ… Skipping already processed: Excerpt 76\n",
      "âœ… Skipping already processed: Excerpt 77\n",
      "âœ… Skipping already processed: Excerpt 78\n",
      "âœ… Skipping already processed: Excerpt 79\n",
      "âœ… Skipping already processed: Excerpt 80\n",
      "âœ… Skipping already processed: Excerpt 81\n",
      "âœ… Skipping already processed: Excerpt 82\n",
      "âœ… Skipping already processed: Excerpt 83\n",
      "âœ… Skipping already processed: Excerpt 84\n",
      "âœ… Skipping already processed: Excerpt 85\n",
      "âœ… Skipping already processed: Excerpt 86\n",
      "âœ… Skipping already processed: Excerpt 87\n",
      "âœ… Skipping already processed: Excerpt 88\n",
      "âœ… Skipping already processed: Excerpt 89\n",
      "âœ… Skipping already processed: Excerpt 90\n",
      "âœ… Skipping already processed: Excerpt 91\n",
      "âœ… Skipping already processed: Excerpt 92\n",
      "âœ… Skipping already processed: Excerpt 93\n",
      "âœ… Skipping already processed: Excerpt 94\n",
      "âœ… Skipping already processed: Excerpt 95\n",
      "âœ… Skipping already processed: Excerpt 96\n",
      "âœ… Skipping already processed: Excerpt 97\n",
      "âœ… Skipping already processed: Excerpt 98\n",
      "âœ… Skipping already processed: Excerpt 99\n",
      "âœ… Skipping already processed: Excerpt 100\n",
      "âœ… Skipping already processed: Excerpt 101\n",
      "âœ… Skipping already processed: Excerpt 102\n",
      "âœ… Skipping already processed: Excerpt 103\n",
      "âœ… Skipping already processed: Excerpt 104\n",
      "âœ… Skipping already processed: Excerpt 105\n",
      "âœ… Skipping already processed: Excerpt 106\n",
      "âœ… Skipping already processed: Excerpt 107\n",
      "âœ… Skipping already processed: Excerpt 108\n",
      "âœ… Skipping already processed: Excerpt 109\n",
      "âœ… Skipping already processed: Excerpt 110\n",
      "âœ… Skipping already processed: Excerpt 111\n",
      "âœ… Skipping already processed: Excerpt 112\n",
      "âœ… Skipping already processed: Excerpt 113\n",
      "âœ… Skipping already processed: Excerpt 114\n",
      "âœ… Skipping already processed: Excerpt 115\n",
      "âœ… Skipping already processed: Excerpt 116\n",
      "âœ… Skipping already processed: Excerpt 117\n",
      "âœ… Skipping already processed: Excerpt 118\n",
      "âœ… Skipping already processed: Excerpt 119\n",
      "âœ… Skipping already processed: Excerpt 120\n",
      "âœ… Skipping already processed: Excerpt 121\n",
      "âœ… Skipping already processed: Excerpt 122\n",
      "âœ… Skipping already processed: Excerpt 123\n",
      "âœ… Skipping already processed: Excerpt 124\n",
      "âœ… Skipping already processed: Excerpt 125\n",
      "âœ… Skipping already processed: Excerpt 126\n",
      "âœ… Skipping already processed: Excerpt 127\n",
      "âœ… Skipping already processed: Excerpt 128\n",
      "âœ… Skipping already processed: Excerpt 129\n",
      "âœ… Skipping already processed: Excerpt 130\n",
      "âœ… Skipping already processed: Excerpt 131\n",
      "âœ… Skipping already processed: Excerpt 132\n",
      "âœ… Skipping already processed: Excerpt 133\n",
      "âœ… Skipping already processed: Excerpt 134\n",
      "âœ… Skipping already processed: Excerpt 135\n",
      "âœ… Skipping already processed: Excerpt 136\n",
      "âœ… Skipping already processed: Excerpt 137\n",
      "âœ… Skipping already processed: Excerpt 138\n",
      "âœ… Skipping already processed: Excerpt 139\n",
      "âœ… Skipping already processed: Excerpt 140\n",
      "âœ… Skipping already processed: Excerpt 141\n",
      "âœ… Skipping already processed: Excerpt 142\n",
      "âœ… Skipping already processed: Excerpt 143\n",
      "âœ… Skipping already processed: Excerpt 144\n",
      "âœ… Skipping already processed: Excerpt 145\n",
      "âœ… Skipping already processed: Excerpt 146\n",
      "ðŸ”Ž Processing Excerpt 147 (147/1509)...\n",
      "ðŸ”„ Processing batch: [('Israeli', 'Novo - Ogaryovo'), ('Israeli', '##av Gallant'), ('Israeli', 'Iraqi'), ('Israeli', 'Yemen'), ('Israeli', 'Iran')]\n",
      "ðŸ“¤ Sending request to GPT-4o (Attempt 1)...\n",
      "âŒ API request failed (Attempt 1): Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}\n",
      "ðŸ“¤ Sending request to GPT-4o (Attempt 2)...\n",
      "âŒ API request failed (Attempt 2): Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}\n",
      "ðŸ“¤ Sending request to GPT-4o (Attempt 3)...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[73], line 334\u001b[0m\n\u001b[1;32m    332\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m entity_batches:\n\u001b[1;32m    333\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mðŸ”„ Processing batch: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbatch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 334\u001b[0m     relationships \u001b[38;5;241m=\u001b[39m \u001b[43mextract_relationships_gpt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    335\u001b[0m     all_relationships\u001b[38;5;241m.\u001b[39mextend(relationships)\n\u001b[1;32m    337\u001b[0m new_entry \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    338\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexcerpt\u001b[39m\u001b[38;5;124m\"\u001b[39m: excerpt_name,\n\u001b[1;32m    339\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrelationships\u001b[39m\u001b[38;5;124m\"\u001b[39m: all_relationships\n\u001b[1;32m    340\u001b[0m }\n",
      "Cell \u001b[0;32mIn[73], line 142\u001b[0m, in \u001b[0;36mextract_relationships_gpt\u001b[0;34m(text, entity_batch, max_retries, timeout)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    141\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mðŸ“¤ Sending request to GPT-4o (Attempt \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mattempt\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 142\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompletions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    143\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgpt-4o\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    144\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrole\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msystem\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m}\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    145\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.3\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    146\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[1;32m    147\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    148\u001b[0m     response_content \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mchoices[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mmessage\u001b[38;5;241m.\u001b[39mcontent\n\u001b[1;32m    149\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mâœ… GPT-4o response received.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Desktop/Uni/Competitions/SMU Datathon/.venv/lib/python3.12/site-packages/openai/_utils/_utils.py:279\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    277\u001b[0m             msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    278\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[0;32m--> 279\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Uni/Competitions/SMU Datathon/.venv/lib/python3.12/site-packages/openai/resources/chat/completions.py:863\u001b[0m, in \u001b[0;36mCompletions.create\u001b[0;34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, reasoning_effort, response_format, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    821\u001b[0m \u001b[38;5;129m@required_args\u001b[39m([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m], [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    822\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcreate\u001b[39m(\n\u001b[1;32m    823\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    860\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m NOT_GIVEN,\n\u001b[1;32m    861\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatCompletion \u001b[38;5;241m|\u001b[39m Stream[ChatCompletionChunk]:\n\u001b[1;32m    862\u001b[0m     validate_response_format(response_format)\n\u001b[0;32m--> 863\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    864\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/chat/completions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    865\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    866\u001b[0m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m    867\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmessages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    868\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    869\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43maudio\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43maudio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    870\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfrequency_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    871\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunction_call\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    872\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunctions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    873\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogit_bias\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    874\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    875\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_completion_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_completion_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    876\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    877\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmetadata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    878\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodalities\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodalities\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mn\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    880\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mparallel_tool_calls\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    881\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprediction\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprediction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    882\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpresence_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    883\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mreasoning_effort\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mreasoning_effort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    884\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mresponse_format\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    885\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseed\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    886\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mservice_tier\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mservice_tier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    887\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstop\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    888\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstore\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    889\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    890\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    891\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtemperature\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    892\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtool_choice\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    893\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtools\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    894\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_logprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    895\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_p\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    896\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    897\u001b[0m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    898\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCompletionCreateParams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    899\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    900\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    901\u001b[0m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[1;32m    902\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    903\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    904\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    905\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    906\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Uni/Competitions/SMU Datathon/.venv/lib/python3.12/site-packages/openai/_base_client.py:1283\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1269\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mpost\u001b[39m(\n\u001b[1;32m   1270\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1271\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1278\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1279\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[1;32m   1280\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[1;32m   1281\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[1;32m   1282\u001b[0m     )\n\u001b[0;32m-> 1283\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/Desktop/Uni/Competitions/SMU Datathon/.venv/lib/python3.12/site-packages/openai/_base_client.py:960\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    957\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    958\u001b[0m     retries_taken \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m--> 960\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    961\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    962\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    963\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    964\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    965\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries_taken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    966\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Uni/Competitions/SMU Datathon/.venv/lib/python3.12/site-packages/openai/_base_client.py:996\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[1;32m    993\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSending HTTP Request: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, request\u001b[38;5;241m.\u001b[39mmethod, request\u001b[38;5;241m.\u001b[39murl)\n\u001b[1;32m    995\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 996\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    997\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    998\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_should_stream_response_body\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    999\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1000\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1001\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m httpx\u001b[38;5;241m.\u001b[39mTimeoutException \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m   1002\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEncountered httpx.TimeoutException\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/Desktop/Uni/Competitions/SMU Datathon/.venv/lib/python3.12/site-packages/httpx/_client.py:914\u001b[0m, in \u001b[0;36mClient.send\u001b[0;34m(self, request, stream, auth, follow_redirects)\u001b[0m\n\u001b[1;32m    910\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_timeout(request)\n\u001b[1;32m    912\u001b[0m auth \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_request_auth(request, auth)\n\u001b[0;32m--> 914\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_handling_auth\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    915\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    916\u001b[0m \u001b[43m    \u001b[49m\u001b[43mauth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    917\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    918\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhistory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    919\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    920\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    921\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n",
      "File \u001b[0;32m~/Desktop/Uni/Competitions/SMU Datathon/.venv/lib/python3.12/site-packages/httpx/_client.py:942\u001b[0m, in \u001b[0;36mClient._send_handling_auth\u001b[0;34m(self, request, auth, follow_redirects, history)\u001b[0m\n\u001b[1;32m    939\u001b[0m request \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(auth_flow)\n\u001b[1;32m    941\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 942\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_handling_redirects\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    943\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    944\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    945\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhistory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhistory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    946\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    947\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    948\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/Desktop/Uni/Competitions/SMU Datathon/.venv/lib/python3.12/site-packages/httpx/_client.py:979\u001b[0m, in \u001b[0;36mClient._send_handling_redirects\u001b[0;34m(self, request, follow_redirects, history)\u001b[0m\n\u001b[1;32m    976\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_event_hooks[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequest\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m    977\u001b[0m     hook(request)\n\u001b[0;32m--> 979\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_single_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    980\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    981\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_event_hooks[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
      "File \u001b[0;32m~/Desktop/Uni/Competitions/SMU Datathon/.venv/lib/python3.12/site-packages/httpx/_client.py:1014\u001b[0m, in \u001b[0;36mClient._send_single_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m   1009\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m   1010\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttempted to send an async request with a sync Client instance.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1011\u001b[0m     )\n\u001b[1;32m   1013\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request\u001b[38;5;241m=\u001b[39mrequest):\n\u001b[0;32m-> 1014\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mtransport\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1016\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response\u001b[38;5;241m.\u001b[39mstream, SyncByteStream)\n\u001b[1;32m   1018\u001b[0m response\u001b[38;5;241m.\u001b[39mrequest \u001b[38;5;241m=\u001b[39m request\n",
      "File \u001b[0;32m~/Desktop/Uni/Competitions/SMU Datathon/.venv/lib/python3.12/site-packages/httpx/_transports/default.py:250\u001b[0m, in \u001b[0;36mHTTPTransport.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    237\u001b[0m req \u001b[38;5;241m=\u001b[39m httpcore\u001b[38;5;241m.\u001b[39mRequest(\n\u001b[1;32m    238\u001b[0m     method\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mmethod,\n\u001b[1;32m    239\u001b[0m     url\u001b[38;5;241m=\u001b[39mhttpcore\u001b[38;5;241m.\u001b[39mURL(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    247\u001b[0m     extensions\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mextensions,\n\u001b[1;32m    248\u001b[0m )\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[0;32m--> 250\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_pool\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resp\u001b[38;5;241m.\u001b[39mstream, typing\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m    254\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Response(\n\u001b[1;32m    255\u001b[0m     status_code\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mstatus,\n\u001b[1;32m    256\u001b[0m     headers\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[1;32m    257\u001b[0m     stream\u001b[38;5;241m=\u001b[39mResponseStream(resp\u001b[38;5;241m.\u001b[39mstream),\n\u001b[1;32m    258\u001b[0m     extensions\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mextensions,\n\u001b[1;32m    259\u001b[0m )\n",
      "File \u001b[0;32m~/Desktop/Uni/Competitions/SMU Datathon/.venv/lib/python3.12/site-packages/httpcore/_sync/connection_pool.py:256\u001b[0m, in \u001b[0;36mConnectionPool.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    253\u001b[0m         closing \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_assign_requests_to_connections()\n\u001b[1;32m    255\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_close_connections(closing)\n\u001b[0;32m--> 256\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    258\u001b[0m \u001b[38;5;66;03m# Return the response. Note that in this case we still have to manage\u001b[39;00m\n\u001b[1;32m    259\u001b[0m \u001b[38;5;66;03m# the point at which the response is closed.\u001b[39;00m\n\u001b[1;32m    260\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response\u001b[38;5;241m.\u001b[39mstream, typing\u001b[38;5;241m.\u001b[39mIterable)\n",
      "File \u001b[0;32m~/Desktop/Uni/Competitions/SMU Datathon/.venv/lib/python3.12/site-packages/httpcore/_sync/connection_pool.py:236\u001b[0m, in \u001b[0;36mConnectionPool.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    232\u001b[0m connection \u001b[38;5;241m=\u001b[39m pool_request\u001b[38;5;241m.\u001b[39mwait_for_connection(timeout\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[1;32m    234\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    235\u001b[0m     \u001b[38;5;66;03m# Send the request on the assigned connection.\u001b[39;00m\n\u001b[0;32m--> 236\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    237\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpool_request\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\n\u001b[1;32m    238\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    239\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ConnectionNotAvailable:\n\u001b[1;32m    240\u001b[0m     \u001b[38;5;66;03m# In some cases a connection may initially be available to\u001b[39;00m\n\u001b[1;32m    241\u001b[0m     \u001b[38;5;66;03m# handle a request, but then become unavailable.\u001b[39;00m\n\u001b[1;32m    242\u001b[0m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m    243\u001b[0m     \u001b[38;5;66;03m# In this case we clear the connection and try again.\u001b[39;00m\n\u001b[1;32m    244\u001b[0m     pool_request\u001b[38;5;241m.\u001b[39mclear_connection()\n",
      "File \u001b[0;32m~/Desktop/Uni/Competitions/SMU Datathon/.venv/lib/python3.12/site-packages/httpcore/_sync/connection.py:103\u001b[0m, in \u001b[0;36mHTTPConnection.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connect_failed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    101\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[0;32m--> 103\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_connection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Uni/Competitions/SMU Datathon/.venv/lib/python3.12/site-packages/httpcore/_sync/http11.py:136\u001b[0m, in \u001b[0;36mHTTP11Connection.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    134\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m Trace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse_closed\u001b[39m\u001b[38;5;124m\"\u001b[39m, logger, request) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[1;32m    135\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_response_closed()\n\u001b[0;32m--> 136\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
      "File \u001b[0;32m~/Desktop/Uni/Competitions/SMU Datathon/.venv/lib/python3.12/site-packages/httpcore/_sync/http11.py:106\u001b[0m, in \u001b[0;36mHTTP11Connection.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Trace(\n\u001b[1;32m     98\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreceive_response_headers\u001b[39m\u001b[38;5;124m\"\u001b[39m, logger, request, kwargs\n\u001b[1;32m     99\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[1;32m    100\u001b[0m     (\n\u001b[1;32m    101\u001b[0m         http_version,\n\u001b[1;32m    102\u001b[0m         status,\n\u001b[1;32m    103\u001b[0m         reason_phrase,\n\u001b[1;32m    104\u001b[0m         headers,\n\u001b[1;32m    105\u001b[0m         trailing_data,\n\u001b[0;32m--> 106\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_receive_response_headers\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    107\u001b[0m     trace\u001b[38;5;241m.\u001b[39mreturn_value \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    108\u001b[0m         http_version,\n\u001b[1;32m    109\u001b[0m         status,\n\u001b[1;32m    110\u001b[0m         reason_phrase,\n\u001b[1;32m    111\u001b[0m         headers,\n\u001b[1;32m    112\u001b[0m     )\n\u001b[1;32m    114\u001b[0m network_stream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_network_stream\n",
      "File \u001b[0;32m~/Desktop/Uni/Competitions/SMU Datathon/.venv/lib/python3.12/site-packages/httpcore/_sync/http11.py:177\u001b[0m, in \u001b[0;36mHTTP11Connection._receive_response_headers\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    174\u001b[0m timeout \u001b[38;5;241m=\u001b[39m timeouts\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mread\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 177\u001b[0m     event \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_receive_event\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(event, h11\u001b[38;5;241m.\u001b[39mResponse):\n\u001b[1;32m    179\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/Uni/Competitions/SMU Datathon/.venv/lib/python3.12/site-packages/httpcore/_sync/http11.py:217\u001b[0m, in \u001b[0;36mHTTP11Connection._receive_event\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    214\u001b[0m     event \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_h11_state\u001b[38;5;241m.\u001b[39mnext_event()\n\u001b[1;32m    216\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m event \u001b[38;5;129;01mis\u001b[39;00m h11\u001b[38;5;241m.\u001b[39mNEED_DATA:\n\u001b[0;32m--> 217\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_network_stream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    218\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mREAD_NUM_BYTES\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[1;32m    219\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    221\u001b[0m     \u001b[38;5;66;03m# If we feed this case through h11 we'll raise an exception like:\u001b[39;00m\n\u001b[1;32m    222\u001b[0m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m    223\u001b[0m     \u001b[38;5;66;03m#     httpcore.RemoteProtocolError: can't handle event type\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    227\u001b[0m     \u001b[38;5;66;03m# perspective. Instead we handle this case distinctly and treat\u001b[39;00m\n\u001b[1;32m    228\u001b[0m     \u001b[38;5;66;03m# it as a ConnectError.\u001b[39;00m\n\u001b[1;32m    229\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;241m==\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_h11_state\u001b[38;5;241m.\u001b[39mtheir_state \u001b[38;5;241m==\u001b[39m h11\u001b[38;5;241m.\u001b[39mSEND_RESPONSE:\n",
      "File \u001b[0;32m~/Desktop/Uni/Competitions/SMU Datathon/.venv/lib/python3.12/site-packages/httpcore/_backends/sync.py:128\u001b[0m, in \u001b[0;36mSyncStream.read\u001b[0;34m(self, max_bytes, timeout)\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m map_exceptions(exc_map):\n\u001b[1;32m    127\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sock\u001b[38;5;241m.\u001b[39msettimeout(timeout)\n\u001b[0;32m--> 128\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_bytes\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.12/3.12.8/Frameworks/Python.framework/Versions/3.12/lib/python3.12/ssl.py:1232\u001b[0m, in \u001b[0;36mSSLSocket.recv\u001b[0;34m(self, buflen, flags)\u001b[0m\n\u001b[1;32m   1228\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1229\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1230\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m   1231\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[0;32m-> 1232\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuflen\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1233\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1234\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv(buflen, flags)\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.12/3.12.8/Frameworks/Python.framework/Versions/3.12/lib/python3.12/ssl.py:1105\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1103\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m, buffer)\n\u001b[1;32m   1104\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1105\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m SSLError \u001b[38;5;28;01mas\u001b[39;00m x:\n\u001b[1;32m   1107\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m x\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m==\u001b[39m SSL_ERROR_EOF \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msuppress_ragged_eofs:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import openai\n",
    "import pandas as pd\n",
    "import json\n",
    "import time\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import re\n",
    "import ast\n",
    "import itertools\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "client = openai.OpenAI(api_key=os.getenv(\"API_KEY\"))\n",
    "\n",
    "# âœ… LOAD THE DATASET\n",
    "file_path = \"results/generalized_normalized_ner_with_embeddings.xlsx\"  # Update with your actual file path\n",
    "try:\n",
    "    df = pd.read_excel(file_path)\n",
    "    print(f\"âœ… Loaded dataset with {len(df)} rows.\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error loading dataset: {e}\")\n",
    "    exit()\n",
    "\n",
    "# âœ… Load Semantic Similarity Model\n",
    "print(\"ðŸ”„ Loading sentence embedding model for category mapping...\")\n",
    "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "print(\"âœ… Model loaded successfully.\")\n",
    "\n",
    "# âœ… Predefined Relationship Categories\n",
    "# standard_relationships = {\n",
    "#     \"legal_violation\": [\"legal\", \"lawsuit\", \"court_case\", \"regulatory_violation\"],\n",
    "#     \"corporate_action\": [\"business_decision\", \"merger\", \"acquisition\"],\n",
    "#     \"government_regulation\": [\"regulation\", \"policy\", \"law_enforcement\"],\n",
    "#     \"geographical_association\": [\"location\", \"place\", \"geographical\"],\n",
    "#     \"crime_involvement\": [\"criminal\", \"crime\", \"fraud\", \"arrest\", \"indictment\"],\n",
    "#     \"financial_transaction\": [\"finance\", \"funding\", \"money_transfer\", \"seizure\", \"investment\"],\n",
    "#     \"scientific_discovery\": [\"science\", \"discovery\", \"innovation\", \"technology_breakthrough\"],\n",
    "#     \"historical_event\": [\"history\", \"historical_event\", \"war\", \"political_event\"]\n",
    "# }\n",
    "\n",
    "def load_standard_relationships():\n",
    "    relations_file = \"standard_relationships.json\"\n",
    "    \"\"\"Loads the relationship dictionary from a JSON file or initializes a default version.\"\"\"\n",
    "    if os.path.exists(relations_file) and os.path.getsize(relations_file) > 0:\n",
    "        try:\n",
    "            with open(relations_file, \"r\", encoding=\"utf-8\") as f:\n",
    "                return json.load(f)\n",
    "        except json.JSONDecodeError:\n",
    "            print(\"âš ï¸ Relationship dictionary corrupted. Resetting to default.\")\n",
    "    return {\n",
    "        \"legal_violation\": [\"legal\", \"lawsuit\", \"court_case\", \"regulatory_violation\"],\n",
    "        \"corporate_action\": [\"business_decision\", \"merger\", \"acquisition\"],\n",
    "        \"government_regulation\": [\"regulation\", \"policy\", \"law_enforcement\"],\n",
    "        \"geographical_association\": [\"location\", \"place\", \"geographical\"],\n",
    "        \"crime_involvement\": [\"criminal\", \"crime\", \"fraud\", \"arrest\", \"indictment\"],\n",
    "        \"financial_transaction\": [\"finance\", \"funding\", \"money_transfer\", \"seizure\", \"investment\"],\n",
    "        \"scientific_discovery\": [\"science\", \"discovery\", \"innovation\", \"technology_breakthrough\"],\n",
    "        \"historical_event\": [\"history\", \"historical_event\", \"war\", \"political_event\"]\n",
    "    }\n",
    "\n",
    "# âœ… Load Relationship Categories\n",
    "standard_relationships = load_standard_relationships()\n",
    "\n",
    "\n",
    "# âœ… Function to Save Updated Relationship Dictionary\n",
    "def save_standard_relationships():\n",
    "    relations_file = \"standard_relationships.json\"\n",
    "    \"\"\"Saves the updated relationship dictionary to a JSON file without duplicates.\"\"\"\n",
    "    # âœ… Remove duplicates before saving\n",
    "    for key in standard_relationships:\n",
    "        standard_relationships[key] = list(set(standard_relationships[key]))  # âœ… Convert to unique list\n",
    "\n",
    "    with open(relations_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(standard_relationships, f, indent=4, ensure_ascii=False)\n",
    "    \n",
    "    print(f\"âœ… Updated relationship dictionary saved to {relations_file}\")\n",
    "\n",
    "\n",
    "# âœ… Function to Group Similar Relationship Types\n",
    "def map_to_standard_relationship(relationship_type):\n",
    "    \"\"\"Maps relationship types to standard categories using semantic similarity.\"\"\"\n",
    "    \n",
    "    # âœ… Ensure relationship_type is a string\n",
    "    if not isinstance(relationship_type, str):\n",
    "        print(f\"âš ï¸ Warning: Converting {relationship_type} to string.\")\n",
    "        relationship_type = str(relationship_type)\n",
    "\n",
    "    relationship_type = relationship_type.lower().replace(\"_\", \" \")\n",
    "    best_match = None\n",
    "    best_score = 0.7  \n",
    "\n",
    "    for standard, variations in standard_relationships.items():\n",
    "        variations = list(set(variations))  # âœ… Ensure variations are unique before comparison\n",
    "\n",
    "        # âœ… Encode embeddings safely\n",
    "        embeddings1 = embedding_model.encode([relationship_type], convert_to_tensor=True)  # **Wrapped in a list**\n",
    "        embeddings2 = embedding_model.encode(variations, convert_to_tensor=True)\n",
    "\n",
    "        similarities = util.pytorch_cos_sim(embeddings1, embeddings2).squeeze().tolist()\n",
    "\n",
    "        # âœ… Ensure similarities is a list before using max()\n",
    "        if isinstance(similarities, float):  \n",
    "            similarities = [similarities]\n",
    "\n",
    "        max_similarity = max(similarities)\n",
    "\n",
    "        if max_similarity > best_score:\n",
    "            best_match = standard\n",
    "            best_score = max_similarity\n",
    "\n",
    "    if best_match is None:\n",
    "        best_match = relationship_type.replace(\" \", \"_\")\n",
    "        standard_relationships.setdefault(best_match, set()).add(relationship_type)  # âœ… Use set to avoid duplicates\n",
    "        print(f\"ðŸ”„ New relationship type detected and added: {best_match}\")\n",
    "        save_standard_relationships()  # âœ… Save updated dictionary\n",
    "\n",
    "    # âœ… Convert variations back to a unique list before saving\n",
    "    standard_relationships[best_match] = list(set(standard_relationships[best_match]))  \n",
    "\n",
    "    return best_match\n",
    "\n",
    "def split_entities(entities, batch_size=5):\n",
    "    entity_names = [e[0] for e in entities]  # Extract entity names only\n",
    "    all_combinations = [pair for pair in itertools.combinations(entity_names, 2) if pair[0] != pair[1]]\n",
    "    \"\"\"Splits entities into smaller batches for better API performance.\"\"\"\n",
    "    return [all_combinations[i:i + batch_size] for i in range(0, len(all_combinations), batch_size)]\n",
    "\n",
    "# âœ… Function to Generate All Possible Entity Pairs\n",
    "def get_entity_pairs(entities):\n",
    "    \"\"\"Returns all possible unique entity pairs.\"\"\"\n",
    "    return list(itertools.combinations([e[0] for e in entities], 2))\n",
    "\n",
    "def extract_relationships_gpt(text, entity_batch, max_retries=3, timeout=15):\n",
    "    \"\"\"Sends multiple API requests, each handling a subset of entity pairs.\"\"\"\n",
    "    \n",
    "    prompt = generate_prompt(text, entity_batch)\n",
    "\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            print(f\"ðŸ“¤ Sending request to GPT-4o (Attempt {attempt+1})...\")\n",
    "            response = client.chat.completions.create(\n",
    "                model=\"gpt-4o\",\n",
    "                messages=[{\"role\": \"system\", \"content\": prompt}],\n",
    "                temperature=0.3,\n",
    "                timeout=timeout\n",
    "            )\n",
    "            response_content = response.choices[0].message.content\n",
    "            print(\"âœ… GPT-4o response received.\")\n",
    "\n",
    "            # ðŸ” Debugging: Print Raw GPT-4o Response\n",
    "            print(f\"ðŸ“ Raw GPT-4o Response:\\n{response_content}\")\n",
    "\n",
    "            # âœ… Extract JSON from markdown block if present\n",
    "            match = re.search(r'```json\\n(.*?)\\n```', response_content, re.DOTALL)\n",
    "            if match:\n",
    "                response_content = match.group(1)\n",
    "\n",
    "            extracted_relationships = json.loads(response_content)\n",
    "\n",
    "            # âœ… Ensure response is always a list\n",
    "            if isinstance(extracted_relationships, dict):\n",
    "                extracted_relationships = [extracted_relationships]\n",
    "\n",
    "            # âœ… Check and convert `relationship_type` to string\n",
    "            for rel in extracted_relationships:\n",
    "                for relationship_entry in rel[\"relationships\"]:\n",
    "                    if not isinstance(relationship_entry[\"relationship_type\"], str):\n",
    "                        print(f\"âš ï¸ Warning: Converting {relationship_entry['relationship_type']} to string.\")\n",
    "                        relationship_entry[\"relationship_type\"] = str(relationship_entry[\"relationship_type\"])\n",
    "\n",
    "                    relationship_entry[\"relationship_type\"] = map_to_standard_relationship(\n",
    "                        relationship_entry[\"relationship_type\"]\n",
    "                    )\n",
    "\n",
    "            return extracted_relationships\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"âŒ Error parsing GPT-4o response as JSON (Attempt {attempt+1}): {e}\")\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ API request failed (Attempt {attempt+1}): {e}\")\n",
    "\n",
    "        time.sleep(2)\n",
    "\n",
    "    return []\n",
    "\n",
    "# âœ… Function to Generate Prompt for GPT-4o\n",
    "def generate_prompt(text, entity_pairs):\n",
    "    \"\"\"Generates the prompt for GPT-4o, processing only a subset of entity pairs.\"\"\"\n",
    "    entity_pairs_text = \"\\n\".join([f\"- {pair[0]} â†” {pair[1]}\" for pair in entity_pairs])\n",
    "\n",
    "    return f\"\"\"\n",
    "You are an expert in **relationship extraction**. Your task is to extract and classify **all possible relationships** between named entities in the given text.\n",
    "\n",
    "---\n",
    "\n",
    "### **ðŸ”¹ Instructions:**\n",
    "1. **Extract relationships for these specific entity pairs:**  \n",
    "{entity_pairs_text}\n",
    "\n",
    "2. **Extract ALL possible and unique relationships between ALL entity pairs given above**  \n",
    "   - **Identify direct relationships** (clearly stated in the text).  \n",
    "   - **Identify indirect relationships** (that can be inferred based on real-world knowledge or logical reasoning).  \n",
    "   - **Consider connections through third entities** (e.g., A relates to C because of B).  \n",
    "   - **Capture contextual relationships** (even if the entities are not explicitly linked in a single sentence).  \n",
    "   - **If no relationship exists, return `\"relationship_type\": \"null\"`, but only if there is absolutely NO logical connection.**  \n",
    "\n",
    "3. **Ensure the output is ALWAYS a JSON ARRAY `[]`, even if there is only one relationship.**  \n",
    "\n",
    "4. **Format the Output as JSON ONLY:**  \n",
    "   Each entity pair should be structured as follows:  \n",
    "   - `\"head\"`: The first entity in the relationship.  \n",
    "   - `\"tail\"`: The second entity in the relationship.  \n",
    "   - `\"relationships\"`: A **list** containing **all relationships** between the two entities.  \n",
    "\n",
    "---\n",
    "\n",
    "### **ðŸ“Œ Example Output Format (Strict JSON Array):**\n",
    "```json\n",
    "[\n",
    "  {{\n",
    "    \"head\": \"Entity 1\",\n",
    "    \"tail\": \"Entity 2\",\n",
    "    \"relationships\": [\n",
    "      {{\n",
    "        \"relationship\": \"First relationship description\",\n",
    "        \"relationship_type\": \"First relationship category\"\n",
    "      }},\n",
    "      {{\n",
    "        \"relationship\": \"Second relationship description\",\n",
    "        \"relationship_type\": \"Second relationship category\"\n",
    "      }}\n",
    "    ]\n",
    "  }},\n",
    "  {{\n",
    "    \"head\": \"Entity 3\",\n",
    "    \"tail\": \"Entity 4\",\n",
    "    \"relationships\": [\n",
    "      {{\n",
    "        \"relationship\": \"Third relationship description\",\n",
    "        \"relationship_type\": \"Third relationship category\"\n",
    "      }}\n",
    "    ]\n",
    "  }}\n",
    "]\n",
    "\n",
    "Return ONLY a JSON array ([]), NOT a JSON object ({{}}).\n",
    "\n",
    "--- \n",
    "\n",
    "Text to Analyze:{text}\n",
    "\n",
    "Entities: {entities}\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def convert_entities_string(entities_str):\n",
    "    try:\n",
    "        # âœ… Convert string to a Python list using `ast.literal_eval()`\n",
    "        entities = ast.literal_eval(entities_str)\n",
    "\n",
    "        # âœ… Ensure the output is a list of lists (convert tuples if needed)\n",
    "        if isinstance(entities, list) and all(isinstance(e, tuple) for e in entities):\n",
    "            entities = [list(e) for e in entities]  # Convert tuples to lists\n",
    "\n",
    "        return entities\n",
    "    except (SyntaxError, ValueError) as e:\n",
    "        print(f\"âŒ Error parsing entities string: {e}\")\n",
    "        return []\n",
    "\n",
    "def load_existing_results(output_file):\n",
    "    \"\"\"Loads existing results from the JSON file to resume from where the script left off.\"\"\"\n",
    "    if os.path.exists(output_file) and os.path.getsize(output_file) > 0:\n",
    "        try:\n",
    "            with open(output_file, \"r\", encoding=\"utf-8\") as f:\n",
    "                existing_results = json.load(f)\n",
    "                return existing_results\n",
    "        except json.JSONDecodeError:\n",
    "            print(\"âš ï¸ JSON file is corrupted or empty. Starting fresh...\")\n",
    "            return []\n",
    "    return []\n",
    "\n",
    "# âœ… Function to save results incrementally (appends new data)\n",
    "def save_results_incrementally(output_file, new_entry):\n",
    "    \"\"\"Appends new extracted relationships to the JSON file after each iteration.\"\"\"\n",
    "    existing_results = load_existing_results(output_file)\n",
    "\n",
    "    # âœ… Check if the excerpt has already been processed (skip duplicates)\n",
    "    processed_excerpts = {entry[\"excerpt\"] for entry in existing_results}\n",
    "    if new_entry[\"excerpt\"] in processed_excerpts:\n",
    "        print(f\"âœ… Skipping already processed: {new_entry['excerpt']}\")\n",
    "        return\n",
    "\n",
    "    # âœ… Append the new entry and save\n",
    "    existing_results.append(new_entry)\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(existing_results, f, indent=4, ensure_ascii=False)\n",
    "    print(f\"âœ… Progress saved to {output_file}\")\n",
    "\n",
    "# âœ… Output file path\n",
    "output_file = \"results/extracted_relationships_multiple.json\"\n",
    "\n",
    "# âœ… Load existing results to resume from where it left off\n",
    "existing_results = load_existing_results(output_file)\n",
    "\n",
    "# âœ… Extract already processed excerpts to avoid re-processing\n",
    "processed_excerpts = {entry[\"excerpt\"] for entry in existing_results}\n",
    "\n",
    "results = existing_results  # Continue appending to existing results\n",
    "\n",
    "# âœ… Main Loop\n",
    "for index, row in df.iterrows():\n",
    "    text = row.get(\"Text\", \"\").strip()\n",
    "    entities = row.get(\"Normalized_NER_Entities\", \"[]\").strip()\n",
    "\n",
    "    if not text or not entities:\n",
    "        print(f\"âš ï¸ Skipping row {index + 1} due to missing data.\")\n",
    "        continue\n",
    "\n",
    "    excerpt_name = f\"Excerpt {index + 1}\"\n",
    "\n",
    "    # âœ… Skip if already processed\n",
    "    if excerpt_name in processed_excerpts:\n",
    "        print(f\"âœ… Skipping already processed: {excerpt_name}\")\n",
    "        continue\n",
    "\n",
    "    print(f\"ðŸ”Ž Processing {excerpt_name} ({index + 1}/{len(df)})...\")\n",
    "\n",
    "    entity_batches = split_entities(convert_entities_string(entities), batch_size=5)\n",
    "\n",
    "    all_relationships = []\n",
    "    for batch in entity_batches:\n",
    "        print(f\"ðŸ”„ Processing batch: {batch}\")\n",
    "        relationships = extract_relationships_gpt(text, batch)\n",
    "        all_relationships.extend(relationships)\n",
    "\n",
    "    new_entry = {\n",
    "        \"excerpt\": excerpt_name,\n",
    "        \"relationships\": all_relationships\n",
    "    }\n",
    "\n",
    "    # âœ… Save results incrementally\n",
    "    save_results_incrementally(output_file, new_entry)\n",
    "\n",
    "    time.sleep(2)\n",
    "\n",
    "print(f\"âœ… Relationship extraction complete! Results saved to {output_file}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
