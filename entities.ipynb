{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoModelForTokenClassification, AutoTokenizer, pipeline\n",
    "\n",
    "# Load the pre-trained NER model from Hugging Face\n",
    "model_name = \"dslim/bert-base-NER\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_name)\n",
    "\n",
    "# Create a Named Entity Recognition pipeline\n",
    "nlp_pipeline = pipeline(\"ner\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "# Load the dataset\n",
    "file_path = \"news_excerpts_parsed.xlsx\"  # Update with your actual file path\n",
    "df = pd.read_excel(file_path)\n",
    "\n",
    "# Function to perform NER on text\n",
    "def extract_ner_entities(text):\n",
    "    if isinstance(text, str):  # Ensure the input is a string\n",
    "        ner_results = nlp_pipeline(text)\n",
    "        extracted_entities = [(entity['word'], entity['entity'], entity['score']) for entity in ner_results]\n",
    "        return extracted_entities\n",
    "    return []\n",
    "\n",
    "# Apply NER extraction to each text entry\n",
    "df[\"NER_Entities\"] = df[\"Text\"].apply(extract_ner_entities)\n",
    "\n",
    "# Save results to a new Excel file\n",
    "df.to_excel(\"ner_extracted_results.xlsx\", index=False)\n",
    "\n",
    "print(\"NER extraction complete. Results saved to 'ner_extracted_results.xlsx'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoModelForTokenClassification, AutoTokenizer, pipeline\n",
    "\n",
    "# Load the pre-trained NER model from Hugging Face\n",
    "model_name = \"dslim/bert-base-NER\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_name)\n",
    "\n",
    "# Create a Named Entity Recognition pipeline (with auto-aggregation)\n",
    "nlp_pipeline = pipeline(\"ner\", model=model, tokenizer=tokenizer, aggregation_strategy=\"simple\")\n",
    "\n",
    "# Load the dataset\n",
    "file_path = \"news_excerpts_parsed.xlsx\"  # Update with actual path\n",
    "df = pd.read_excel(file_path)\n",
    "\n",
    "# Function to extract unique NER entities with the highest confidence score\n",
    "def extract_unique_ner_entities(text):\n",
    "    if isinstance(text, str):  # Ensure input is a valid string\n",
    "        ner_results = nlp_pipeline(text)\n",
    "        \n",
    "        entity_dict = {}  # Dictionary to track the highest confidence score per entity\n",
    "        for entity in ner_results:\n",
    "            entity_name = entity['word']\n",
    "            entity_type = entity['entity_group']\n",
    "            confidence = round(entity['score'], 4)\n",
    "            \n",
    "            # Store only the highest confidence occurrence\n",
    "            if entity_name not in entity_dict or confidence > entity_dict[entity_name][1]:\n",
    "                entity_dict[entity_name] = (entity_type, confidence)\n",
    "\n",
    "        # Convert back to list format: [(Entity, Type, Confidence)]\n",
    "        return [(name, details[0], details[1]) for name, details in entity_dict.items()]\n",
    "    \n",
    "    return []\n",
    "\n",
    "# Apply the function to extract and filter unique NER entities\n",
    "df[\"Final_NER_Entities\"] = df[\"Text\"].apply(extract_unique_ner_entities)\n",
    "\n",
    "# Save results to a new Excel file\n",
    "df.to_excel(\"cleaned_ner_results.xlsx\", index=False)\n",
    "\n",
    "print(\"✅ NER extraction complete. Duplicates removed. Results saved to 'cleaned_ner_results.xlsx'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from fuzzywuzzy import process, fuzz\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "# Load the dataset\n",
    "file_path = \"cleaned_ner_results.xlsx\"  # Update this with the actual file path\n",
    "df = pd.read_excel(file_path)\n",
    "\n",
    "print(\"✅ 1. Load a sentence transformer model for semantic similarity\")\n",
    "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "print(\"✅ 2. Extract all unique entity names dynamically from the dataset\")\n",
    "all_entities = set()\n",
    "for entity_list in df[\"Final_NER_Entities\"]:\n",
    "    for name, entity_type, _ in eval(entity_list):  # Convert string to list\n",
    "        all_entities.add(name)\n",
    "\n",
    "print(\"✅ 3. Compute word embeddings for all entity names using SentenceTransformer\")\n",
    "entity_embeddings = {\n",
    "    entity: embedding_model.encode(entity, convert_to_tensor=True)\n",
    "    for entity in all_entities\n",
    "}\n",
    "\n",
    "print(\"✅ 4. Automatically generate alias mappings using fuzzy matching and semantic similarity\")\n",
    "def build_alias_map(entities, threshold_fuzzy=85, threshold_semantic=0.85):\n",
    "    \"\"\"\n",
    "    - Uses fuzzy matching to find similar text-based entities.\n",
    "    - Uses cosine similarity to find semantically similar entities.\n",
    "    - Groups variations under a single canonical entity name.\n",
    "    \"\"\"\n",
    "    alias_map = {}\n",
    "    processed = set()\n",
    "    entity_list = list(entities)\n",
    "    print(\"Number of entities to complete: \", len(entity_list))\n",
    "\n",
    "    for i, entity in enumerate(entity_list):\n",
    "        print(\"Completed entity: \", i)\n",
    "        if entity in processed:\n",
    "            continue\n",
    "\n",
    "        # ✅ 4.1 Fuzzy matching to find textually similar entities\n",
    "        match, score = process.extractOne(entity, entity_list[i+1:]) if entity_list[i+1:] else (None, 0)\n",
    "\n",
    "        # ✅ 4.2 Compute semantic similarity (cosine similarity of embeddings)\n",
    "        best_semantic_match = None\n",
    "        best_semantic_score = 0\n",
    "\n",
    "        for other_entity in entity_list:\n",
    "            if entity == other_entity:\n",
    "                continue\n",
    "            semantic_score = util.pytorch_cos_sim(\n",
    "                entity_embeddings[entity], entity_embeddings[other_entity]\n",
    "            ).item()\n",
    "            if semantic_score > best_semantic_score:\n",
    "                best_semantic_match = other_entity\n",
    "                best_semantic_score = semantic_score\n",
    "\n",
    "        # ✅ 4.3 Choose the best match (either fuzzy or semantic)\n",
    "        best_match = None\n",
    "        if match and score >= threshold_fuzzy:\n",
    "            best_match = match\n",
    "        if best_semantic_match and best_semantic_score >= threshold_semantic:\n",
    "            best_match = best_semantic_match\n",
    "\n",
    "        # ✅ 4.4 If a valid match is found, group them under the longer/more formal name\n",
    "        if best_match:\n",
    "            canonical_name = max(entity, best_match, key=len)  # Keep the longer/more descriptive name\n",
    "            alias_map[entity] = canonical_name\n",
    "            alias_map[best_match] = canonical_name\n",
    "            processed.add(entity)\n",
    "            processed.add(best_match)\n",
    "\n",
    "    return alias_map\n",
    "\n",
    "print(\"✅ 5. Generate the alias mapping dynamically\")\n",
    "entity_aliases = build_alias_map(all_entities)\n",
    "\n",
    "print(\"✅ 6. Function to apply alias mapping\")\n",
    "def normalize_entity(entity_name):\n",
    "    return entity_aliases.get(entity_name, entity_name)  # Replace if alias exists\n",
    "\n",
    "print(\"✅ 7. Apply alias mapping to NER entities\")\n",
    "df[\"Normalized_NER_Entities\"] = df[\"Final_NER_Entities\"].apply(\n",
    "    lambda entities: [\n",
    "        (normalize_entity(name), entity_type, confidence)\n",
    "        for name, entity_type, confidence in eval(entities)\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(\"✅ 8. Save standardized entity results to a new file\")\n",
    "df.to_excel(\"generalized_normalized_ner_with_embeddings.xlsx\", index=False)\n",
    "print(\"✅ Auto-aliasing with word embeddings & fuzzy matching complete!\")\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# ✅ RELATIONSHIP EXTRACTION (AFTER ENTITY STANDARDIZATION)\n",
    "# -------------------------------------------------------------\n",
    "\n",
    "# ✅ 9. Function to extract relationships while ensuring standardization\n",
    "def extract_relationships(text, entities):\n",
    "    \"\"\"\n",
    "    - Extracts relationships between entities within the same excerpt.\n",
    "    - Uses standardized entity names.\n",
    "    \"\"\"\n",
    "    relationships = []\n",
    "    entity_names = [name for name, _, _ in entities]  # Extract only entity names\n",
    "    \n",
    "    # ✅ 9.1 Generate simple entity-entity relationships (for each entity pair)\n",
    "    for i in range(len(entity_names)):\n",
    "        for j in range(i + 1, len(entity_names)):\n",
    "            relationships.append((entity_names[i], entity_names[j]))  # (Entity1, Entity2)\n",
    "    \n",
    "    return relationships\n",
    "\n",
    "# ✅ 10. Apply relationship extraction using normalized entity names\n",
    "df[\"Relationships\"] = df.apply(\n",
    "    lambda row: extract_relationships(row[\"Text\"], row[\"Normalized_NER_Entities\"]),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# ✅ 11. Aggregate relationships across all excerpts\n",
    "relationship_dict = defaultdict(set)\n",
    "\n",
    "for relationships in df[\"Relationships\"]:\n",
    "    for entity1, entity2 in relationships:\n",
    "        relationship_dict[entity1].add(entity2)\n",
    "        relationship_dict[entity2].add(entity1)  # Bidirectional relationship\n",
    "\n",
    "# ✅ 12. Convert relationships into a DataFrame for better visualization\n",
    "relationship_df = pd.DataFrame(\n",
    "    [(k, list(v)) for k, v in relationship_dict.items()], \n",
    "    columns=[\"Entity\", \"Related Entities\"]\n",
    ")\n",
    "\n",
    "# ✅ 13. Save relationships to a new Excel file\n",
    "relationship_df.to_excel(\"aggregated_relationships.xlsx\", index=False)\n",
    "print(\"✅ Relationship extraction complete! Results saved to 'aggregated_relationships.xlsx'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from fuzzywuzzy import process, fuzz\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "# Load the dataset\n",
    "file_path = \"cleaned_ner_results.xlsx\"  # Update this with the actual file path\n",
    "df = pd.read_excel(file_path)\n",
    "\n",
    "# ✅ 1. Load the sentence transformer model (Fast, 6x speedup)\n",
    "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# ✅ 2. Extract all unique entity names dynamically from the dataset\n",
    "all_entities = set()\n",
    "for entity_list in df[\"Final_NER_Entities\"]:  # Change this if your column name is different\n",
    "    for name, entity_type, _ in eval(entity_list):  # Convert string to list\n",
    "        all_entities.add(name)\n",
    "\n",
    "# ✅ 3. Compute embeddings for all entity names at once (100x faster than loops)\n",
    "entity_list = list(all_entities)\n",
    "entity_embeddings = embedding_model.encode(entity_list, convert_to_tensor=True)\n",
    "\n",
    "# ✅ 4. Use clustering to pre-group similar entities (Agglomerative Clustering)\n",
    "def cluster_entities(entities, embeddings, threshold=0.85):\n",
    "    \"\"\"\n",
    "    - Uses Agglomerative Clustering to group similar entities\n",
    "    - Reduces the number of comparisons needed\n",
    "    \"\"\"\n",
    "    clustering = AgglomerativeClustering(n_clusters=None, distance_threshold=1 - threshold, linkage='average', metric='cosine')\n",
    "    labels = clustering.fit_predict(embeddings.cpu().numpy())\n",
    "\n",
    "    # Create a mapping from entity to its cluster\n",
    "    cluster_map = defaultdict(list)\n",
    "    for idx, label in enumerate(labels):\n",
    "        cluster_map[label].append(entities[idx])\n",
    "\n",
    "    # Pick the most descriptive name from each cluster\n",
    "    alias_map = {}\n",
    "    for cluster in cluster_map.values():\n",
    "        canonical_name = max(cluster, key=len)  # Choose the longest name as canonical\n",
    "        for entity in cluster:\n",
    "            alias_map[entity] = canonical_name\n",
    "\n",
    "    return alias_map\n",
    "\n",
    "# ✅ 5. Generate the alias mapping using clustering (FAST, scalable)\n",
    "entity_aliases = cluster_entities(entity_list, entity_embeddings)\n",
    "\n",
    "# ✅ 6. Function to normalize entity names using the alias mapping\n",
    "def normalize_entity(entity_name):\n",
    "    return entity_aliases.get(entity_name, entity_name)  # Replace if alias exists\n",
    "\n",
    "# ✅ 7. Apply alias mapping to NER entities\n",
    "df[\"Normalized_NER_Entities\"] = df[\"Final_NER_Entities\"].apply(\n",
    "    lambda entities: [\n",
    "        (normalize_entity(name), entity_type, confidence)\n",
    "        for name, entity_type, confidence in eval(entities)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# ✅ 8. Save standardized entity results to a new file\n",
    "df.to_excel(\"optimized_normalized_ner.xlsx\", index=False)\n",
    "print(\"✅ Auto-aliasing complete! Results saved to 'optimized_normalized_ner.xlsx'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from fuzzywuzzy import process, fuzz\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from transformers import AutoModelForTokenClassification, AutoTokenizer, pipeline\n",
    "\n",
    "# Load the dataset\n",
    "file_path = \"news_excerpts_parsed.xlsx\"  # Update this with the actual file path\n",
    "df = pd.read_excel(file_path)\n",
    "\n",
    "# ########################################################\n",
    "# ✅ 1. Named Entity Recognition (NER) Extraction\n",
    "# ########################################################\n",
    "\n",
    "# Load the pre-trained NER model\n",
    "model_name = \"dslim/bert-base-NER\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_name)\n",
    "\n",
    "# Create a Named Entity Recognition pipeline\n",
    "nlp_pipeline = pipeline(\"ner\", model=model, tokenizer=tokenizer, aggregation_strategy=\"simple\")\n",
    "\n",
    "# ✅ Function to perform NER extraction & merge subwords (### tokens)\n",
    "def extract_ner_entities(text):\n",
    "    if isinstance(text, str):\n",
    "        ner_results = nlp_pipeline(text)\n",
    "        \n",
    "        merged_entities = []\n",
    "        current_entity = []\n",
    "        current_label = None\n",
    "\n",
    "        for entity in ner_results:\n",
    "            word = entity['word']\n",
    "            entity_type = entity['entity_group']\n",
    "            confidence = round(entity['score'], 4)\n",
    "\n",
    "            # Handle subword tokens (e.g., 'U.', '##OB' -> \"UOB\")\n",
    "            if word.startswith(\"##\"):\n",
    "                if current_entity:\n",
    "                    current_entity[-1] += word[2:]  # Merge subword into previous token\n",
    "            else:\n",
    "                if current_entity:\n",
    "                    merged_entities.append((\"\".join(current_entity), current_label, max_confidence))\n",
    "                current_entity = [word]\n",
    "                current_label = entity_type\n",
    "                max_confidence = confidence\n",
    "\n",
    "        # Add the last entity if present\n",
    "        if current_entity:\n",
    "            merged_entities.append((\"\".join(current_entity), current_label, max_confidence))\n",
    "\n",
    "        return merged_entities\n",
    "    return []\n",
    "\n",
    "# Apply NER extraction to each text entry\n",
    "df[\"NER_Entities\"] = df[\"Text\"].apply(extract_ner_entities)\n",
    "\n",
    "# ########################################################\n",
    "# ✅ 2. Entity Standardization (Fuzzy Matching + Clustering)\n",
    "# ########################################################\n",
    "\n",
    "# ✅ Load the sentence transformer model for entity similarity\n",
    "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# ✅ Extract all unique entity names dynamically from the dataset\n",
    "all_entities = set()\n",
    "for entity_list in df[\"NER_Entities\"]:\n",
    "    for name, entity_type, _ in entity_list:\n",
    "        all_entities.add(name)\n",
    "\n",
    "# ✅ Compute embeddings for all entity names (Batch processing)\n",
    "entity_list = list(all_entities)\n",
    "entity_embeddings = embedding_model.encode(entity_list, convert_to_tensor=True)\n",
    "\n",
    "# ✅ Function to cluster entities and standardize names\n",
    "def cluster_entities(entities, embeddings, threshold=0.85):\n",
    "    \"\"\"\n",
    "    - Uses Agglomerative Clustering to group similar entities\n",
    "    - Reduces the number of comparisons needed\n",
    "    \"\"\"\n",
    "    clustering = AgglomerativeClustering(\n",
    "        n_clusters=None, distance_threshold=1 - threshold, linkage='average', metric='cosine'\n",
    "    )\n",
    "    labels = clustering.fit_predict(embeddings.cpu().numpy())\n",
    "\n",
    "    # Create a mapping from entity to its cluster\n",
    "    cluster_map = defaultdict(list)\n",
    "    for idx, label in enumerate(labels):\n",
    "        cluster_map[label].append(entities[idx])\n",
    "\n",
    "    # Pick the most descriptive name from each cluster\n",
    "    alias_map = {}\n",
    "    for cluster in cluster_map.values():\n",
    "        canonical_name = max(cluster, key=len)  # Choose the longest name as canonical\n",
    "        for entity in cluster:\n",
    "            alias_map[entity] = canonical_name\n",
    "\n",
    "    return alias_map\n",
    "\n",
    "# ✅ Generate the alias mapping dynamically\n",
    "entity_aliases = cluster_entities(entity_list, entity_embeddings)\n",
    "\n",
    "# ✅ Function to normalize entity names using alias mapping\n",
    "def normalize_entity(entity_name):\n",
    "    return entity_aliases.get(entity_name, entity_name)\n",
    "\n",
    "# ✅ Apply alias mapping to NER entities\n",
    "df[\"Normalized_NER_Entities\"] = df[\"NER_Entities\"].apply(\n",
    "    lambda entities: [\n",
    "        (normalize_entity(name), entity_type, confidence)\n",
    "        for name, entity_type, confidence in entities\n",
    "    ]\n",
    ")\n",
    "\n",
    "# ✅ Save standardized entity results to a new file\n",
    "df.to_excel(\"optimized_normalized_ner.xlsx\", index=False)\n",
    "print(\"✅ Auto-aliasing complete! Results saved to 'optimized_normalized_ner.xlsx'.\")\n",
    "\n",
    "# ########################################################\n",
    "# ✅ 3. Relationship Extraction (After Entity Standardization)\n",
    "# ########################################################\n",
    "\n",
    "# ✅ Function to extract relationships while ensuring standardization\n",
    "def extract_relationships(text, entities):\n",
    "    \"\"\"\n",
    "    - Extracts relationships between entities within the same excerpt.\n",
    "    - Uses standardized entity names.\n",
    "    \"\"\"\n",
    "    relationships = []\n",
    "    entity_names = [name for name, _, _ in entities]  # Extract only entity names\n",
    "    \n",
    "    # ✅ Generate simple entity-entity relationships (for each entity pair)\n",
    "    for i in range(len(entity_names)):\n",
    "        for j in range(i + 1, len(entity_names)):\n",
    "            relationships.append((entity_names[i], entity_names[j]))  # (Entity1, Entity2)\n",
    "    \n",
    "    return relationships\n",
    "\n",
    "# ✅ Apply relationship extraction using normalized entity names\n",
    "df[\"Relationships\"] = df.apply(\n",
    "    lambda row: extract_relationships(row[\"Text\"], row[\"Normalized_NER_Entities\"]),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# ✅ Aggregate relationships across all excerpts\n",
    "relationship_dict = defaultdict(set)\n",
    "\n",
    "for relationships in df[\"Relationships\"]:\n",
    "    for entity1, entity2 in relationships:\n",
    "        relationship_dict[entity1].add(entity2)\n",
    "        relationship_dict[entity2].add(entity1)  # Bidirectional relationship\n",
    "\n",
    "# ✅ Convert relationships into a DataFrame for better visualization\n",
    "relationship_df = pd.DataFrame(\n",
    "    [(k, list(v)) for k, v in relationship_dict.items()], \n",
    "    columns=[\"Entity\", \"Related Entities\"]\n",
    ")\n",
    "\n",
    "# ✅ Save relationships to a new Excel file\n",
    "relationship_df.to_excel(\"aggregated_relationships.xlsx\", index=False)\n",
    "print(\"✅ Relationship extraction complete! Results saved to 'aggregated_relationships.xlsx'.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
